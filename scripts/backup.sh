#!/bin/bash

###############################################################################
# è³‡æ–™åº«å‚™ä»½è…³æœ¬
###############################################################################
#
# åŠŸèƒ½ï¼š
# - å‚™ä»½ PostgreSQL è³‡æ–™åº«
# - è‡ªå‹•æ¸…ç†è¶…éä¿ç•™æœŸé™çš„èˆŠå‚™ä»½
# - å¯é¸ï¼šä¸Šå‚³åˆ° S3 (éœ€è¦é…ç½® AWS CLI)
#
# ä½¿ç”¨æ–¹æ³•ï¼š
# - æ‰‹å‹•å‚™ä»½ï¼šbash scripts/backup.sh
# - è‡ªå‹•å‚™ä»½ï¼šæ·»åŠ åˆ° crontab
#   æ¯æ—¥å‡Œæ™¨ 2 é»å‚™ä»½ï¼š0 2 * * * /home/opc/projects/kiro-stock-platform/scripts/backup.sh >> /home/opc/projects/kiro-stock-platform/logs/backup.log 2>&1
#
###############################################################################

set -e

# ============================================================================
# é…ç½®
# ============================================================================
PROJECT_DIR="/home/opc/projects/kiro-stock-platform"
BACKUP_DIR="$PROJECT_DIR/backups"
RETENTION_DAYS=30  # ä¿ç•™æœ€è¿‘ 30 å¤©çš„å‚™ä»½
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")

# å¾ .env.production è®€å–è³‡æ–™åº«é…ç½®
if [ -f "$PROJECT_DIR/.env.production" ]; then
    export $(grep -v '^#' $PROJECT_DIR/.env.production | xargs)
else
    echo "âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° .env.production æ–‡ä»¶ï¼"
    exit 1
fi

# è³‡æ–™åº«é…ç½®
DB_NAME="${POSTGRES_DB:-stock_analysis}"
DB_USER="${POSTGRES_USER:-postgres}"
DB_PASSWORD="${POSTGRES_PASSWORD}"
DB_CONTAINER="stock_analysis_db_prod"

# S3 é…ç½® (å¯é¸)
ENABLE_S3_BACKUP=false
S3_BUCKET="${BACKUP_S3_BUCKET}"
S3_REGION="${BACKUP_S3_REGION:-ap-northeast-1}"

# ============================================================================
# å‡½æ•¸å®šç¾©
# ============================================================================

# æ—¥èªŒå‡½æ•¸
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
}

# æª¢æŸ¥ Docker å®¹å™¨æ˜¯å¦é‹è¡Œ
check_container() {
    if ! docker ps | grep -q $DB_CONTAINER; then
        log "âŒ éŒ¯èª¤ï¼šè³‡æ–™åº«å®¹å™¨ $DB_CONTAINER æœªé‹è¡Œï¼"
        exit 1
    fi
}

# å‚™ä»½è³‡æ–™åº«
backup_database() {
    local backup_file="$BACKUP_DIR/db_backup_${TIMESTAMP}.sql.gz"

    log "ğŸ”„ é–‹å§‹å‚™ä»½è³‡æ–™åº«: $DB_NAME"

    docker exec $DB_CONTAINER pg_dump \
        -U $DB_USER \
        -d $DB_NAME \
        --no-owner \
        --no-acl \
        --clean \
        --if-exists \
        | gzip > "$backup_file"

    if [ $? -eq 0 ]; then
        local size=$(du -h "$backup_file" | cut -f1)
        log "âœ… è³‡æ–™åº«å‚™ä»½æˆåŠŸ: $backup_file (å¤§å°: $size)"
        echo "$backup_file"
    else
        log "âŒ è³‡æ–™åº«å‚™ä»½å¤±æ•—ï¼"
        exit 1
    fi
}

# æ¸…ç†èˆŠå‚™ä»½
cleanup_old_backups() {
    log "ğŸ§¹ æ¸…ç†è¶…é $RETENTION_DAYS å¤©çš„èˆŠå‚™ä»½..."

    local count=$(find $BACKUP_DIR -name "db_backup_*.sql.gz" -mtime +$RETENTION_DAYS | wc -l)

    if [ $count -gt 0 ]; then
        find $BACKUP_DIR -name "db_backup_*.sql.gz" -mtime +$RETENTION_DAYS -delete
        log "âœ… å·²åˆªé™¤ $count å€‹èˆŠå‚™ä»½æ–‡ä»¶"
    else
        log "â„¹ï¸  æ²’æœ‰éœ€è¦æ¸…ç†çš„èˆŠå‚™ä»½"
    fi
}

# ä¸Šå‚³åˆ° S3 (å¯é¸)
upload_to_s3() {
    local backup_file=$1

    if [ "$ENABLE_S3_BACKUP" != "true" ]; then
        return
    fi

    if [ -z "$S3_BUCKET" ]; then
        log "âš ï¸  è­¦å‘Šï¼šS3_BUCKET æœªé…ç½®ï¼Œè·³é S3 ä¸Šå‚³"
        return
    fi

    log "â˜ï¸  ä¸Šå‚³å‚™ä»½åˆ° S3: s3://$S3_BUCKET/backups/"

    if command -v aws &> /dev/null; then
        aws s3 cp "$backup_file" "s3://$S3_BUCKET/backups/" \
            --region $S3_REGION \
            --storage-class STANDARD_IA

        if [ $? -eq 0 ]; then
            log "âœ… S3 ä¸Šå‚³æˆåŠŸ"
        else
            log "âŒ S3 ä¸Šå‚³å¤±æ•—"
        fi
    else
        log "âš ï¸  è­¦å‘Šï¼šAWS CLI æœªå®‰è£ï¼Œè·³é S3 ä¸Šå‚³"
    fi
}

# å‚™ä»½é…ç½®æ–‡ä»¶
backup_configs() {
    local config_backup="$BACKUP_DIR/config_backup_${TIMESTAMP}.tar.gz"

    log "ğŸ“¦ å‚™ä»½é…ç½®æ–‡ä»¶..."

    tar -czf "$config_backup" \
        -C $PROJECT_DIR \
        .env.production \
        docker-compose.prod.yml \
        nginx/nginx.conf \
        nginx/conf.d \
        airflow/airflow.cfg \
        2>/dev/null

    if [ $? -eq 0 ]; then
        local size=$(du -h "$config_backup" | cut -f1)
        log "âœ… é…ç½®æ–‡ä»¶å‚™ä»½æˆåŠŸ: $config_backup (å¤§å°: $size)"
    else
        log "âš ï¸  é…ç½®æ–‡ä»¶å‚™ä»½å¤±æ•—ï¼ˆå¯èƒ½éƒ¨åˆ†æ–‡ä»¶ä¸å­˜åœ¨ï¼‰"
    fi
}

# å‚™ä»½çµ±è¨ˆ
backup_stats() {
    log "ğŸ“Š å‚™ä»½çµ±è¨ˆ:"
    log "   - å‚™ä»½ç›®éŒ„: $BACKUP_DIR"
    log "   - ç¸½å‚™ä»½æ•¸: $(ls -1 $BACKUP_DIR/db_backup_*.sql.gz 2>/dev/null | wc -l)"
    log "   - ç¸½å¤§å°: $(du -sh $BACKUP_DIR | cut -f1)"
    log "   - æœ€æ–°å‚™ä»½: $(ls -1t $BACKUP_DIR/db_backup_*.sql.gz 2>/dev/null | head -1)"
}

# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

main() {
    log "ğŸš€ é–‹å§‹å‚™ä»½æµç¨‹..."

    # å‰µå»ºå‚™ä»½ç›®éŒ„
    mkdir -p $BACKUP_DIR

    # æª¢æŸ¥å®¹å™¨ç‹€æ…‹
    check_container

    # åŸ·è¡Œå‚™ä»½
    backup_file=$(backup_database)

    # å‚™ä»½é…ç½®æ–‡ä»¶
    backup_configs

    # ä¸Šå‚³åˆ° S3 (å¦‚æœå•Ÿç”¨)
    upload_to_s3 "$backup_file"

    # æ¸…ç†èˆŠå‚™ä»½
    cleanup_old_backups

    # é¡¯ç¤ºçµ±è¨ˆ
    backup_stats

    log "ğŸ‰ å‚™ä»½æµç¨‹å®Œæˆï¼"
}

# åŸ·è¡Œä¸»ç¨‹åº
main
