#!/usr/bin/env python3
"""
XCom å¤–éƒ¨å­˜å„²æ¸¬è©¦
"""
import sys
import os
import json
from pathlib import Path

# æ·»åŠ DAGç›®éŒ„åˆ°è·¯å¾‘
dag_path = Path(__file__).parent.parent
sys.path.insert(0, str(dag_path))

from common.storage.xcom_storage import XComStorageManager, store_large_data, retrieve_large_data


def test_xcom_storage():
    """æ¸¬è©¦XComå¤–éƒ¨å­˜å„²åŠŸèƒ½"""
    print("=" * 60)
    print("æ¸¬è©¦XComå¤–éƒ¨å­˜å„²åŠŸèƒ½")
    print("=" * 60)

    try:
        # å‰µå»ºæ¸¬è©¦æ•¸æ“š
        large_stock_data = {
            'items': [
                {
                    'id': i,
                    'symbol': f'STOCK{i:04d}.TW',
                    'market': 'TW',
                    'name': f'æ¸¬è©¦è‚¡ç¥¨{i}',
                    'is_active': True,
                    'price_history': [
                        {'date': '2024-01-01', 'open': 100 + i, 'high': 110 + i, 'low': 95 + i, 'close': 105 + i},
                        {'date': '2024-01-02', 'open': 105 + i, 'high': 115 + i, 'low': 100 + i, 'close': 110 + i},
                        {'date': '2024-01-03', 'open': 110 + i, 'high': 120 + i, 'low': 105 + i, 'close': 115 + i}
                    ]
                }
                for i in range(1, 101)  # 100æ”¯è‚¡ç¥¨
            ],
            'total': 100,
            'page': 1,
            'page_size': 100,
            'total_pages': 1
        }

        # è¨ˆç®—æ•¸æ“šå¤§å°
        data_size = len(json.dumps(large_stock_data, ensure_ascii=False).encode('utf-8'))
        print(f"æ¸¬è©¦æ•¸æ“šå¤§å°: {data_size} bytes ({data_size / 1024:.2f} KB)")

        if data_size < 50000:  # å¦‚æœæ•¸æ“šä¸å¤ å¤§ï¼Œå¢åŠ æ›´å¤šæ•¸æ“š
            print("æ•¸æ“šå¤§å°ä¸è¶³ä»¥æ¸¬è©¦XComé™åˆ¶ï¼Œå¢åŠ æ›´å¤šæ•¸æ“š...")
            for stock in large_stock_data['items']:
                # ç‚ºæ¯æ”¯è‚¡ç¥¨æ·»åŠ æ›´å¤šæ­·å²æ•¸æ“š
                for day in range(4, 100):  # æ·»åŠ æ›´å¤šå¤©çš„æ•¸æ“š
                    stock['price_history'].append({
                        'date': f'2024-01-{day:02d}',
                        'open': 100 + stock['id'] + day,
                        'high': 110 + stock['id'] + day,
                        'low': 95 + stock['id'] + day,
                        'close': 105 + stock['id'] + day,
                        'volume': 1000000 + stock['id'] * 1000 + day * 100
                    })

            data_size = len(json.dumps(large_stock_data, ensure_ascii=False).encode('utf-8'))
            print(f"æ“´å±•å¾Œæ•¸æ“šå¤§å°: {data_size} bytes ({data_size / 1024:.2f} KB)")

        # æ¸¬è©¦å­˜å„²
        print("\næ¸¬è©¦å­˜å„²å¤§æ•¸æ“š...")
        storage_manager = XComStorageManager()

        # æª¢æŸ¥Redisé€£æ¥
        stats = storage_manager.get_storage_stats()
        if 'error' in stats:
            print(f"âŒ Redisé€£æ¥å¤±æ•—: {stats['error']}")
            print("è«‹ç¢ºä¿Redisæœå‹™æ­£åœ¨é‹è¡Œ")
            return False

        print("âœ… Redisé€£æ¥æˆåŠŸ")

        # å­˜å„²æ•¸æ“š
        reference_id = store_large_data(large_stock_data, "test_stock_data_001")
        print(f"âœ… æ•¸æ“šå·²å­˜å„²ï¼Œå¼•ç”¨ID: {reference_id}")

        # æ¸¬è©¦æª¢ç´¢
        print("\næ¸¬è©¦æª¢ç´¢å¤§æ•¸æ“š...")
        retrieved_data = retrieve_large_data(reference_id)
        print("âœ… æ•¸æ“šæª¢ç´¢æˆåŠŸ")

        # é©—è­‰æ•¸æ“šå®Œæ•´æ€§
        print("\né©—è­‰æ•¸æ“šå®Œæ•´æ€§...")
        if retrieved_data == large_stock_data:
            print("âœ… æ•¸æ“šå®Œæ•´æ€§é©—è­‰é€šé")
        else:
            print("âŒ æ•¸æ“šå®Œæ•´æ€§é©—è­‰å¤±æ•—")
            return False

        # æ¸¬è©¦å…ƒæ•¸æ“š
        print("\næ¸¬è©¦å…ƒæ•¸æ“š...")
        metadata = storage_manager.get_metadata(reference_id)
        if metadata:
            print(f"âœ… å…ƒæ•¸æ“šç²å–æˆåŠŸ:")
            print(f"   å‰µå»ºæ™‚é–“: {metadata.get('created_at')}")
            print(f"   æ•¸æ“šå¤§å°: {metadata.get('data_size')} bytes")
            print(f"   TTL: {metadata.get('ttl_seconds')} ç§’")
        else:
            print("âŒ å…ƒæ•¸æ“šç²å–å¤±æ•—")

        # æ¸¬è©¦å­˜å„²çµ±è¨ˆ
        print("\nå­˜å„²çµ±è¨ˆä¿¡æ¯...")
        stats = storage_manager.get_storage_stats()
        print(f"âœ… å­˜å„²é …ç›®æ•¸: {stats.get('total_items', 0)}")
        print(f"âœ… ç¸½å¤§å°: {stats.get('total_size_mb', 0)} MB")

        # æ¸¬è©¦æ¸…ç†
        print("\næ¸¬è©¦æ•¸æ“šæ¸…ç†...")
        from common.storage.xcom_storage import cleanup_large_data
        cleanup_success = cleanup_large_data(reference_id)
        if cleanup_success:
            print("âœ… æ•¸æ“šæ¸…ç†æˆåŠŸ")
        else:
            print("âŒ æ•¸æ“šæ¸…ç†å¤±æ•—")

        # é©—è­‰æ¸…ç†çµæœ
        try:
            retrieve_large_data(reference_id)
            print("âŒ æ¸…ç†é©—è­‰å¤±æ•—ï¼šæ•¸æ“šä»ç„¶å­˜åœ¨")
            return False
        except ValueError:
            print("âœ… æ¸…ç†é©—è­‰é€šéï¼šæ•¸æ“šå·²è¢«åˆªé™¤")

        print("\nğŸ‰ æ‰€æœ‰XComå¤–éƒ¨å­˜å„²æ¸¬è©¦éƒ½é€šéäº†ï¼")
        return True

    except Exception as e:
        print(f"âŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_xcom_size_limits():
    """æ¸¬è©¦XComå¤§å°é™åˆ¶"""
    print("\n" + "=" * 60)
    print("æ¸¬è©¦XComå¤§å°é™åˆ¶")
    print("=" * 60)

    # å‰µå»ºä¸åŒå¤§å°çš„æ•¸æ“šä¾†æ¸¬è©¦é–¾å€¼
    test_sizes = [
        (1000, "1KB"),
        (10000, "10KB"),
        (40000, "40KB"),
        (50000, "50KB"),
        (100000, "100KB")
    ]

    for size, description in test_sizes:
        # å‰µå»ºæŒ‡å®šå¤§å°çš„æ•¸æ“š
        test_data = {
            'data': 'x' * (size - 100),  # ç•™ä¸€äº›ç©ºé–“çµ¦JSONçµæ§‹
            'size_description': description,
            'item_count': size // 100
        }

        actual_size = len(json.dumps(test_data, ensure_ascii=False).encode('utf-8'))
        print(f"\næ¸¬è©¦ {description} æ•¸æ“š (å¯¦éš›: {actual_size} bytes):")

        try:
            # æ¨¡æ“¬APIæ“ä½œå™¨çš„é‚è¼¯
            max_xcom_size = 40960  # 40KB
            if actual_size > max_xcom_size:
                print(f"  ğŸ”„ æ•¸æ“šå¤§å°è¶…é {max_xcom_size} bytesï¼Œä½¿ç”¨å¤–éƒ¨å­˜å„²")
                ref_id = store_large_data(test_data)
                result = {
                    'external_storage': True,
                    'reference_id': ref_id,
                    'data_size': actual_size,
                    'summary': {'type': 'dict', 'keys': list(test_data.keys())}
                }
                print(f"  âœ… å¤–éƒ¨å­˜å„²æˆåŠŸï¼Œå¼•ç”¨ID: {ref_id}")

                # æ¸…ç†
                cleanup_large_data(ref_id)
            else:
                print(f"  âœ… æ•¸æ“šå¤§å°åœ¨XComé™åˆ¶å…§ï¼Œç›´æ¥è¿”å›")
                result = test_data

        except Exception as e:
            print(f"  âŒ æ¸¬è©¦å¤±æ•—: {e}")

    print("\nğŸ‰ XComå¤§å°é™åˆ¶æ¸¬è©¦å®Œæˆï¼")


if __name__ == "__main__":
    success = test_xcom_storage()
    if success:
        test_xcom_size_limits()

    sys.exit(0 if success else 1)